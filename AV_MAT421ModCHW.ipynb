{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNXV0V6goOh1jJG4sFL53Oz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AJ-4-beep/MAT-421/blob/main/AV_MAT421ModCHW.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Submit your Jupyter Notebook files to demonstrate the following concepts:\n",
        "\n",
        "\n",
        "\n",
        "Root Finding Problem Statement\n",
        "\n",
        "Tolerance\n",
        "\n",
        "Bisection Method\n",
        "\n",
        "Newton-Raphson Method"
      ],
      "metadata": {
        "id": "teLYQiTJp_na"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Root finding Problem Statement: The root/zero of a function is when, given a certain input value, it outputs 0. Finding the roots of certain equations is easier than others. For f(x) = x^2 - 9, its real roots are 3 and -3. When more complicated expressions are involved, it becomes harder to find roots, like for the equation f(x) = cos(x) - x. Using code, we can better estimate the roots to these equations, as shown below:"
      ],
      "metadata": {
        "id": "6Nuq0VEWqBdH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy import optimize\n",
        "\n",
        "f = lambda x: np.cos(x) - x\n",
        "r = optimize.fsolve(f, -2)\n",
        "\n",
        "\n",
        "print(\"r =\", r)\n",
        "\n",
        "\n",
        "result = f(r)\n",
        "print(\"result=\", result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cN_57yYKhW2O",
        "outputId": "a7137769-4cf4-4959-9bd3-36926b8bce06"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "r = [0.73908513]\n",
            "result= [0.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code above estimates the value of the root of the equation when x=-2. As expected, the result is close enough to 0 such that the computed value can be considered a root of the equation."
      ],
      "metadata": {
        "id": "LnLkKDMThhWf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tolerance: In any computing scenario, there will be error, like with the previously mentioned float value (couple homeworks ago), and, because of this, we cannot expect perfectly accurate answers from a machine. Thus, the idea of tolerance is established; we know things will have error, so to accept an answer as valid for a given query, we establish a tolerance that allows a little bit of variation, to account for the amount of error we expect.\n",
        "\n",
        "Essentially, tolerance is a way to establish that an answer is \"good enough\" or \"close enough\" to the real answer. This is especially helpful in the above problem, where finding roots is both time consuming and complicated."
      ],
      "metadata": {
        "id": "eHELqqpqiEsS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To give an example, let error be represented by the equation e = abs(f(x)), and f(x) = x + tol/2, where tol represents tolerance, e for error, and abs as an absolute value operation.\n",
        "\n",
        "f(0), in this case, would result in f(x) = tol/2. As a result, error would be, by its equation, smaller than the tolerance value. Therefore, because the ouput value is smaller than the tolerance, 0 would be a valid root for this equation."
      ],
      "metadata": {
        "id": "ycZVwX-HjDUl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bisection Method: The Intermediate Value Theorem states that f(x) is continuous between a and b, and the sign(f(a)) != sign(f(b)), then there must exist value c such that a < c < b and f(c) = 0.\n",
        "\n",
        "Essentially, if a function's output values cross from positive to negative (or vice versa), and its continous, somewhere it has to have a value that equals 0, where it actually crosses over and changes sign.\n",
        "\n",
        "This theorem can be used iteratively, to get closer and closer to this zero value, which is in fact a root of f(x). By consistently establishing a and b, and their midpoint m, we can establish what f(m) is. If f(m) is within tolerance, we've found a root. If not, we can shift a and b to be smaller or larger, and try again, until f(m) is within tolerance. This can be established in code."
      ],
      "metadata": {
        "id": "Yk_vj3xNjovC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def my_bisection(f, a, b, tol):\n",
        "\n",
        "  if np.sign(f(a)) == np.sign(f(b)):\n",
        "    raise Exception(\n",
        "    \"The scalars a and b do not bound a root\")\n",
        "\n",
        "\n",
        "  m = (a + b)/2\n",
        "\n",
        "  if np.abs(f(m)) < tol:\n",
        "\n",
        "    return m\n",
        "\n",
        "  elif np.sign(f(a)) == np.sign(f(m)):\n",
        "\n",
        "    return my_bisection(f, m, b, tol)\n",
        "\n",
        "  elif np.sign(f(b)) == np.sign(f(m)):\n",
        "\n",
        "    return my_bisection(f, a, m, tol)"
      ],
      "metadata": {
        "id": "K0Njhq1Bl4hU"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The above code defines a bisection algorithm; m is established to be the endpoint of 2 different values a and b (that must have different signs). If f(m) is within tolerance, the root has been found. If f(m) shares the sign of a, the midpoint is too close to a, and therefore, the equation is rerun, but now with m as the a value. Essentially, if f(m) is evalutated and m turns out to be too close to a, we shift it to be closer to b, by choosing to continue evaluating from m to b rather than a to b. A similar process is done if m is too close to b, and the equation is ran until f(m) is within tolerance."
      ],
      "metadata": {
        "id": "fC61IQIBmQjG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "f = lambda x: x**2 - 2\n",
        "r1 = my_bisection(f, 0, 2, 0.1)\n",
        "print(\"r1 =\", r1)\n",
        "r01 = my_bisection(f, 0, 2, 0.01)\n",
        "print(\"r01 =\", r01)\n",
        "print(\"f(r1) =\", f(r1))\n",
        "print(\"f(r01) =\", f(r01))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "svZOIlhDnIgz",
        "outputId": "c1d6e1b4-e666-47a6-a8cd-977dcdad5c92"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "r1 = 1.4375\n",
            "r01 = 1.4140625\n",
            "f(r1) = 0.06640625\n",
            "f(r01) = -0.00042724609375\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The above uses the defined function to establish that the square root of 2 can be estimated as a solution to the function f(x) = x^2 - 2, starting with a value 0 and b value 2, with a tolerance established where f(x) must be less than .1, and then again, with it being less that .01. As seen, when ran with a tighter tolerance, the value becomes more accurate."
      ],
      "metadata": {
        "id": "yczD6w-GnNZK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Newton-Raphson Method: This is a root finding method that also iteratively establishes newer and newer x values based on previously established values. Starting at x0 as a given value, this method constructs new x values, which then construct new x values, and so on, with the following equation as basis: xi = xi-1  -   (g(xi-1))   /  (g'(xi-1))"
      ],
      "metadata": {
        "id": "gQlbVmU7nyX3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The new x value equals the previous value minus g(previousXvalue) divided by gprime(previousXvalue), where gprime(x) is the differential of g(x) with respect to x. Essentially, this method is based on using the differential as the main driving force for estimation, and can be modeled in code."
      ],
      "metadata": {
        "id": "kknpQzaJoqmN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "f = lambda x: x**2 - 2\n",
        "f_prime = lambda x: 2*x\n",
        "newton_raphson = 1.4 - (f(1.4))/(f_prime(1.4))\n",
        "print(\"newton_raphson =\", newton_raphson)\n",
        "print(\"sqrt(2) =\", np.sqrt(2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Es0yNimepFbW",
        "outputId": "fb234fd2-7ceb-4bc8-c1a5-a8813a27cda0"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "newton_raphson = 1.4142857142857144\n",
            "sqrt(2) = 1.4142135623730951\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Comparing the output of this method to the calculated ouput of the sqrt of 2, one can see that they are pretty close. With some more code, this can become more modular."
      ],
      "metadata": {
        "id": "Sis2FYd4pH-j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def my_newton(f, df, x0, tol):\n",
        "\n",
        "  if abs(f(x0)) < tol:\n",
        "   return x0\n",
        "  else:\n",
        "    return my_newton(f, df, x0 - f(x0)/df(x0), tol)"
      ],
      "metadata": {
        "id": "sz7Qj4DSpTm6"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we establish the basic equation for estimation."
      ],
      "metadata": {
        "id": "dJPVxWePpcUU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "estimate = my_newton(f, f_prime, 1.5, 1e-6)\n",
        "print(\"estimate =\", estimate)\n",
        "print(\"sqrt(2) =\", np.sqrt(2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3nUe3v0hpiyJ",
        "outputId": "f56b4ac0-a71a-4044-94f5-314398098fb0"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "estimate = 1.4142135623746899\n",
            "sqrt(2) = 1.4142135623730951\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using that equation, we can estimate what the sqrt2 is, given a starting point of x0 = 1.5, with a tolerance of 1e-6. Comparing to the previous equation, we can see that this is more accurate, as the tolerance provided was tighter."
      ],
      "metadata": {
        "id": "6hT5S9IfplJG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The newton rhapson method is faster than the bisection method, given that the starting x0 is close to the final x value. This would require that there is some knowledge about where the final x value is expected to be; this method is faster only if one can reasonably estimate an initial value that should be pretty close to the final value."
      ],
      "metadata": {
        "id": "hHkblAPSp70_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Python has different root solving functions, most popular of which is probably fsolve from scipy."
      ],
      "metadata": {
        "id": "Wio4kZ-2q0Hv"
      }
    }
  ]
}