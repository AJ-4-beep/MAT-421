{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPo0TyNj2xFrMIPdPxVQz4B",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AJ-4-beep/MAT-421/blob/main/AJModDHW.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Submit your Jupyter Notebook files to demonstrate the following concepts:  (Note: section numbers and topics from notes and not from the textbook)\n",
        "\n",
        "Linear Algebra concepts such as linear space, orthogonality, eigenvalues.\n",
        "\n",
        "Linear regression."
      ],
      "metadata": {
        "id": "eMrf9Rmed7-r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Linear Space: A linear space is built up of linear combinations of vectors. A linear combination of a vectors is a set of vectors multiplied by some scalar value to generate a new set of vectors.\n",
        "\n",
        "Vectors are linearly independent if they cannot be written as another combination of vectors; if a vector cannot be written as another vector multiplied by a scalar, they are independent."
      ],
      "metadata": {
        "id": "R6XFw7vqd9Ny"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "vector = np.array([1,1,2])\n",
        "vector2 = np.array([-2,-2,-4])\n",
        "\n",
        "vector = vector * -2\n",
        "\n",
        "\n",
        "print(vector)\n",
        "print(vector2)\n",
        "print( vector == vector2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y6aIYSVd74AA",
        "outputId": "4be3f60b-d5ac-4910-b298-4b22288a3bc7"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[-2 -2 -4]\n",
            "[-2 -2 -4]\n",
            "[ True  True  True]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The above code shows the idea of vectors that are linearly dependent, which is the opposite of being linearly dependent. The 2 vectors are initially (1,1,2) and (-2,-2,-4), which are evidently different from each other. However, by multiplying the first vector by the scalar -2, the first vector becomes (-2,-2,-4), which is equivalent to the second vector. Thus, these vectors are linearly dependent."
      ],
      "metadata": {
        "id": "YNn7RlnNAE7f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Orthogonality: In simple terms, orthagonality can be thought of as two perpendicular vectors, or vectors that will only cross at one point.\n",
        "\n",
        "The idea of orthogonality is especially important with vectors and linear spaces because the orthogonal vectors help build orthogonal bases, which help build linear spaces out of small unit vectors. Essentially, orthogonal vectors, when reduced to their smallest form, a unit vector, can help build a linear space where all of the vectors are inherently linearly independent; this is useful because solutions to problems in this linear space will be linearly independent. That is to say that solutions in this space will be unique and identifiable.\n",
        "\n",
        "A list of unit vectors that are pairwise orthogonal will therefore be named orthonormal."
      ],
      "metadata": {
        "id": "ms9wZ9pgByBd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Eigenvalues: Eigenvalues are values such that Ax = (lambda)x, where x represents some eigenvector, A is a matrix, and lambda is the eigenvalue. Essentially, it is a value that represents a solution equal to the matrix presented to you, and is useful when attempting to transform matrices.\n",
        "\n",
        "However, not all matrices will have eigenvalues and eigenvectors."
      ],
      "metadata": {
        "id": "6uuEQ_9XLnc8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from numpy.linalg import eig\n",
        "\n",
        "x = np.array([[2,4], [-4, -8]])\n",
        "w,v = eig(x)\n",
        "\n",
        "print('Eigenvalue:', w)\n",
        "print('Eigenvector:', v)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sTjiaDb_WSIZ",
        "outputId": "263c3d4a-5fc8-4143-c805-5a439a53dd31"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Eigenvalue: [ 0. -6.]\n",
            "Eigenvector: [[ 0.89442719 -0.4472136 ]\n",
            " [-0.4472136   0.89442719]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As seen above, a square matrix with vectors (2,4) and (-4,-8) is made, and the eignevectors/values are found with the eig function. Essentially, eigenvalues and eigenvectors are a method to help transform matrices into different forms."
      ],
      "metadata": {
        "id": "EvFAheATW3fQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Linear Regression: Linear regression is, generally, a simple method to establish a linear relationship between data. QR decomposition is one method that can be used to decompose a matrix into 2 different matrices, that, when multiplied together, yield the original matrix."
      ],
      "metadata": {
        "id": "9Eoib1F3hart"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from numpy.linalg import qr\n",
        "\n",
        "A = np.array([[2,6], [8,14]])\n",
        "\n",
        "Q,R = qr(A)\n",
        "print('Q:', Q)\n",
        "print('R:', R)\n",
        "\n",
        "y = np.dot(Q,R)\n",
        "print('QR:', y)\n",
        "\n",
        "iterations = [1,20]\n",
        "for i in range(20):\n",
        "  Q,R = qr(x)\n",
        "  x = np.dot(R,Q)\n",
        "  if i+1 in iterations:\n",
        "    print(f'Iteration {i+1}:')\n",
        "    print(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JAFtMjWTldT0",
        "outputId": "1f551e61-4661-4464-9ff6-590ec5ac48f6"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Q: [[-0.24253563 -0.9701425 ]\n",
            " [-0.9701425   0.24253563]]\n",
            "R: [[ -8.24621125 -15.03720875]\n",
            " [  0.          -2.42535625]]\n",
            "QR: [[ 2.  6.]\n",
            " [ 8. 14.]]\n",
            "Iteration 1:\n",
            "[[-6.00000000e+00 -8.00000000e+00]\n",
            " [-7.94410929e-16 -3.97205465e-16]]\n",
            "Iteration 20:\n",
            "[[-6.00000000e+000  8.00000000e+000]\n",
            " [-5.14737352e-319  6.62009108e-016]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The above code takes in a matrix A, and generates two matrices Q and R in order to help find their decomposed matrices. The overall function of this is to help generate simpler matrices that can assist in the creation of a linear regression model. The problem of the \"least squares\" method is resolved by using the QR method to help convert the matrices' data into coefficients for the overall equation."
      ],
      "metadata": {
        "id": "Rn5cdr70o1Dk"
      }
    }
  ]
}